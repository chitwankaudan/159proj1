{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An analysis of the State of the Union speeches - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import shelve\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "plt.style.use('seaborn-dark')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading some of the data created in the previous part, so we can continue where we left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses = pd.read_hdf('results/df1.h5', 'addresses')\n",
    "with shelve.open('results/vars1') as db:\n",
    "    speeches = db['speeches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check that we're getting the full set of speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(addresses.shape)\n",
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text analysis\n",
    "\n",
    "Let's ask a few basic questions about this text, by populating our `addresses` dataframe with some extra information. As a reminder, so far we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1790-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1790-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1791-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1792-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1793-12-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             president                        title       date\n",
       "0    George Washington   State of the Union Address 1790-01-08\n",
       "1    George Washington   State of the Union Address 1790-12-08\n",
       "2    George Washington   State of the Union Address 1791-10-25\n",
       "3    George Washington   State of the Union Address 1792-11-06\n",
       "4    George Washington   State of the Union Address 1793-12-03"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the following information to this DF:\n",
    "\n",
    "* `n_words`: number of words in the speech\n",
    "* `n_uwords`: number of *unique* words in the speech\n",
    "* `n_swords`: number of *unique, stemmed* words in the speech\n",
    "* `n_chars`: number of letters in the speech\n",
    "* `n_sent`: number of sentences in the speech\n",
    "\n",
    "For this level of complexity, it's probably best if we go with NLTK. Remember, that `speeches` is our list with all the speeches, indexed in the same way as the `addresses` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def clean_word_tokenize(doc):\n",
    "    \"\"\"custom word tokenizer which removes stop words and punctuation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : string\n",
    "        A document to be tokenized\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tokens\n",
    "    \"\"\"\n",
    "    # combine stop words and punctuation\n",
    "    stop = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    \n",
    "    # filter out stop words and punctuation and send to lower case\n",
    "    tokens = [token.lower() for token in nltk.word_tokenize(doc) if token not in stop]\n",
    "\n",
    "    return(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "len(set(stemmer.stem(word) for word in clean_word_tokenize(speeches[4])))\n",
    "#len([stemmer.stem(word) for word in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(clean_word_tokenize(speeches[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6752"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speeches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute these quantities for each speech, as well as saving the set of unique, stemmed words for each speech, which we'll need later to construct the complete term-document matrix across all speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses['n_sent'] = [len(nltk.sent_tokenize(speech)) for speech in speeches]\n",
    "addresses['n_words_all'] = [len(nltk.word_tokenize(speech)) for speech in speeches]\n",
    "addresses['n_words'] = [len(clean_word_tokenize(speech)) for speech in speeches]\n",
    "addresses['n_uwords'] = [len(set(clean_word_tokenize(speech))) for speech in speeches]\n",
    "\n",
    "\n",
    "\n",
    "addresses['n_swords'] = [len(set(stemmer.stem(clean_word_tokenize(speech))) for speech in speeches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a summary of these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.precision = 2\n",
    "addresses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing characteristics of the speeches\n",
    "\n",
    "Now we explore some of the relationships between the speeches, their authors, and time.\n",
    "\n",
    "How properties of the speeches change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Seaborn to provide a plot such as this, and discuss:\n",
    "plt.savefig(\"fig/speech_changes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the distributions by president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "plt.savefig(\"fig/speech_characteristics.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate results storage\n",
    "\n",
    "Since this may have taken a while, we now serialize the results we have for further use. Note that we don't overwrite our original dataframe file, so we can load both (even though in this notebook we reused the name `addresses`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses.to_hdf('results/df2.h5', 'addresses')\n",
    "\n",
    "with shelve.open('results/vars2') as db:\n",
    "    db['speech_words'] = speech_words\n",
    "    db['speeches_cleaned'] = speeches_cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
